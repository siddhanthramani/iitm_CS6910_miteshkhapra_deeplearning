{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports required\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.char2index = {\"SOS\": 0, \"EOS\": 1}\n",
    "        self.char2count = {}\n",
    "        self.index2char = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_chars = 2  # Count SOS and EOS\n",
    "\n",
    "    def addword(self, word):\n",
    "        # Splits a word into individual characters and adds them to the object instance\n",
    "        for char in list(word):\n",
    "            self.addchar(char)\n",
    "\n",
    "    def addchar(self, char):\n",
    "        # If the char is being encountered for the first time\n",
    "        if char not in self.char2index:\n",
    "            self.char2index[char] = self.n_chars\n",
    "            self.char2count[char] = 1\n",
    "            self.index2char[self.n_chars] = char\n",
    "            # Increment count of unique chars\n",
    "            self.n_chars += 1\n",
    "        else:\n",
    "            # Increment count to depict how many times this char has occured\n",
    "            self.char2count[char] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(data_path, lang1, lang2):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open(data_path, encoding=\"utf-8\").\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[s for s in l.split(\",\")] for l in lines]\n",
    "\n",
    "    # Make Lang instances\n",
    "    input_lang = Lang(lang1)\n",
    "    output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 51200 sentence pairs\n",
      "Counting chars...\n",
      "Counted words:\n",
      "eng 28\n",
      "tam 48\n",
      "['vazhiththedungkal', 'வழித்தெடுங்கள்']\n"
     ]
    }
   ],
   "source": [
    "def  prepareData(data_path, lang1, lang2):\n",
    "    input_lang, output_lang, pairs = readLangs(data_path, lang1, lang2)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting chars...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addword(pair[0])\n",
    "        output_lang.addword(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_chars)\n",
    "    print(output_lang.name, output_lang.n_chars)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData(\"../../data/Aksharantar/aksharantar_sampled/aksharantar_sampled/tam/tam_train.csv\", 'eng', 'tam')\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t': 56253,\n",
       " 'h': 51106,\n",
       " 'o': 16365,\n",
       " 'a': 172788,\n",
       " 'c': 6050,\n",
       " 'r': 36211,\n",
       " 'y': 16109,\n",
       " 'm': 25043,\n",
       " 'e': 20573,\n",
       " 'n': 38419,\n",
       " 'i': 64107,\n",
       " 'v': 21484,\n",
       " 'u': 45117,\n",
       " 'd': 17659,\n",
       " 'g': 14824,\n",
       " 'l': 32680,\n",
       " 'z': 3554,\n",
       " 'k': 40135,\n",
       " 'p': 27530,\n",
       " 's': 10519,\n",
       " 'b': 2550,\n",
       " 'w': 406,\n",
       " 'j': 1488,\n",
       " 'q': 24,\n",
       " 'f': 148,\n",
       " 'x': 68}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_lang.char2count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'த': 41674, 'ொ': 3498, 'ட': 26961, '்': 100092, 'ா': 29506, 'ச': 14466, 'ர': 25180, 'ய': 17666, 'ம': 25036, 'ெ': 5686, 'ன': 19584, 'ை': 16577, 'அ': 3916, 'வ': 21882, 'ற': 12550, 'ி': 44664, 'ு': 42713, 'ஆ': 1219, 'ண': 5762, 'க': 53703, 'ள': 14456, 'ல': 17951, 'ழ': 3727, 'ஒ': 602, 'ந': 8539, 'ே': 7402, 'ப': 30399, 'ூ': 2859, 'ீ': 2631, 'ோ': 6002, 'ஜ': 1184, 'எ': 1300, 'இ': 2441, 'ஹ': 427, 'ங': 3923, 'உ': 1622, 'ஷ': 799, 'ஏ': 364, 'ஞ': 606, 'ஃ': 92, 'ஸ': 1510, 'ஈ': 155, 'ஓ': 229, 'ௌ': 66, 'ஊ': 229, 'ஐ': 66}\n"
     ]
    }
   ],
   "source": [
    "print(output_lang.char2count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device is a cuda device if compatible NVidia GPU is found.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_type_dict = {\n",
    "    \"rnn\" : nn.RNN,\n",
    "    \"gru\" : nn.GRU,\n",
    "    \"lstm\" : nn.LSTM\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, rnn_type, input_embedding_dict_size, input_embedding_size, hidden_size, num_layers=1, dropout=0, bidirectional=False):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.input_embedding_dict_size = input_embedding_dict_size\n",
    "        self.embedding_size = input_embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.bidirectional = bidirectional\n",
    "        self.rnn_type = rnn_type\n",
    "        self.embedding = nn.Embedding(input_embedding_dict_size, input_embedding_size)\n",
    "        self.encoder = rnn_type_dict[rnn_type](input_embedding_size, hidden_size, num_layers, dropout=dropout, bidirectional=bidirectional)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded_input = self.embedding(input).view(1, 1, -1)\n",
    "        print(embedded_input)\n",
    "        print(hidden)\n",
    "        output, hidden = self.encoder(embedded_input, hidden)\n",
    "        print(output.shape, hidden.shape)\n",
    "        if self.rnn_type == \"lstm\":\n",
    "            hidden = hidden[0]\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        if self.bidirectional:\n",
    "            return torch.zeros(2*(self.num_layers), 1, self.hidden_size, device=device)\n",
    "        else:\n",
    "            return torch.zeros(self.num_layers, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, rnn_type, output_embedding_dict_size, output_embedding_size, hidden_size, num_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_embedding_dict_size = output_embedding_dict_size\n",
    "        self.output_embedding_size = output_embedding_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(output_embedding_dict_size, output_embedding_size)\n",
    "        self.rnn_type = rnn_type\n",
    "        self.decoder = rnn_type_dict[rnn_type](output_embedding_size, hidden_size, num_layers)\n",
    "        self.out = nn.Linear(hidden_size, output_embedding_dict_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, output, hidden):\n",
    "        embedded_output = self.embedding(output).view(1, 1, -1)\n",
    "        embedded_output_relued = F.relu(embedded_output)\n",
    "        output, hidden = self.decoder(embedded_output_relued, hidden)\n",
    "        if self.rnn_type == \"lstm\":\n",
    "            hidden = hidden[0]\n",
    "        final_output = self.softmax(self.out(output[0]))\n",
    "        return final_output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.num_layers, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 50\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, rnn_type, output_embedding_dict_size, output_embedding_size, hidden_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_embedding_dict_size = output_embedding_dict_size\n",
    "        self.output_embedding_size = output_embedding_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_embedding_dict_size, self.output_embedding_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.decoder = rnn_type_dict[rnn_type](self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_embedding_dict_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.decoder(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromword(lang, word):\n",
    "    return [lang.char2index[char] for char in list(word)]\n",
    "\n",
    "\n",
    "def tensorFromword(lang, word):\n",
    "    indexes = indexesFromword(lang, word)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromword(input_lang, pair[0])\n",
    "    target_tensor = tensorFromword(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "MAX_LENGTH = 50\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        print(input_tensor[ei].shape, encoder_hidden.shape)\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            # decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "            #     decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            # decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "            #     decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_epochs, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()    \n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_epochs + 1):\n",
    "        for pair in pairs:\n",
    "            training_pair = tensorsFromPair(pair)\n",
    "            input_tensor = training_pair[0]\n",
    "            target_tensor = training_pair[1]\n",
    "\n",
    "            loss = train(input_tensor, target_tensor, encoder,\n",
    "                        decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "\n",
    "            if iter % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_epochs),\n",
    "                                            iter, iter / n_epochs * 100, print_loss_avg))\n",
    "\n",
    "            if iter % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total / plot_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, word, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromword(input_lang, word)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_chars = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            # decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "            #     decoder_input, decoder_hidden, encoder_outputs)\n",
    "            # decoder_attentions[di] = decoder_attention.data\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_chars.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_chars.append(output_lang.index2char[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_chars, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        # output_chars, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_chars = evaluate(encoder, decoder, pair[0])\n",
    "        output_word = ' '.join(output_chars)\n",
    "        print('<', output_chars)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1]) torch.Size([2, 1, 512])\n",
      "tensor([[[-9.3200e-01, -2.9655e-01,  9.4944e-01,  8.5187e-01, -1.3671e+00,\n",
      "           1.5677e-01,  1.9577e+00,  4.2158e-01, -1.1370e-01,  2.9017e-02,\n",
      "          -2.0034e-01,  8.3601e-01,  9.3456e-01, -1.2302e+00,  9.1368e-01,\n",
      "          -8.1910e-01,  2.4626e-01,  8.4059e-01,  3.8462e-01, -8.9721e-01,\n",
      "           4.5189e-01, -1.3602e+00,  6.2090e-01, -3.9616e-01, -1.0500e+00,\n",
      "          -8.5442e-02, -1.7138e+00,  6.1976e-01, -2.3562e-01, -3.9142e-01,\n",
      "           1.8939e-01,  6.1902e-01, -1.1712e+00,  1.0569e+00, -1.2134e+00,\n",
      "           1.1529e-02, -2.5087e+00, -8.2341e-01, -2.4080e-01, -1.9333e+00,\n",
      "          -1.0922e-01,  1.1405e+00,  6.2890e-01,  6.3285e-01,  1.7320e+00,\n",
      "          -3.1443e-01, -7.4816e-01, -7.0465e-01,  6.2300e-01, -3.1082e-01,\n",
      "           1.7104e-01, -6.8860e-01,  3.0501e-01,  1.0623e+00,  1.8416e-01,\n",
      "          -2.5285e-01,  6.5169e-01,  8.9695e-01, -1.1626e+00, -7.6409e-01,\n",
      "           2.6683e+00,  4.9818e-01, -3.3495e-02,  4.1705e-03, -7.4467e-01,\n",
      "          -4.6998e-01, -6.6207e-01, -3.3592e-02,  6.0162e-01,  9.6485e-02,\n",
      "           9.2325e-01, -2.4596e-02, -5.9136e-01,  1.2765e+00, -6.8579e-01,\n",
      "           9.1238e-02,  2.9952e-01,  6.2103e-01, -4.8415e-01, -6.2179e-02,\n",
      "          -9.3213e-01, -2.5251e+00,  2.3240e+00,  5.9035e-02,  1.9781e-01,\n",
      "          -3.6076e-01,  1.7315e-01,  1.5020e+00, -2.1671e-01, -1.9780e-01,\n",
      "           4.7691e-01, -1.1807e+00,  6.5478e-01, -2.0140e-01, -7.9082e-01,\n",
      "          -9.9180e-01,  1.6398e-01, -4.0134e-01, -6.7755e-01, -6.1400e-01,\n",
      "          -8.6220e-01,  1.3247e+00,  1.0131e+00, -9.4984e-02,  1.3287e-01,\n",
      "          -1.1449e+00,  8.2108e-01,  5.2949e-01,  3.7770e-01,  8.4917e-01,\n",
      "           5.3562e-01,  1.4494e+00,  3.1213e-01,  7.8427e-01,  1.5121e-01,\n",
      "           9.3150e-01, -5.2234e-01, -4.5363e-01, -8.0381e-02,  1.4313e+00,\n",
      "          -7.0686e-01, -2.4096e+00, -1.0166e+00,  8.6930e-01, -1.0199e+00,\n",
      "           9.9605e-01, -2.3804e+00, -1.3617e+00,  4.9890e-01, -1.1402e+00,\n",
      "           1.1044e+00,  7.5025e-01, -2.0375e+00, -9.9162e-01, -1.1426e+00,\n",
      "           2.4259e+00,  3.3179e-01,  9.9899e-01,  1.5538e+00,  5.6718e-01,\n",
      "           6.5465e-01,  1.4975e+00, -2.2862e-01,  1.1101e+00, -2.8219e+00,\n",
      "          -2.2533e-01, -9.8526e-01,  1.7613e-01, -2.0086e+00,  5.7200e-01,\n",
      "          -6.6089e-02,  4.7614e-01,  1.8268e-01, -2.1722e-01,  5.0456e-01,\n",
      "          -1.3565e+00, -6.1056e-01,  1.0515e+00, -1.4707e-01, -1.4512e-02,\n",
      "          -3.5218e-01,  6.4922e-01, -4.6613e-02,  1.0670e+00,  7.6002e-01,\n",
      "          -1.8186e+00,  1.4066e+00,  5.3706e-01,  6.5726e-01,  1.5537e-01,\n",
      "           1.0164e-01,  1.8488e-02,  9.3407e-01,  6.7781e-01, -1.4214e+00,\n",
      "          -1.5589e+00,  2.9627e-01, -5.8733e-01, -9.4530e-01,  5.0474e-01,\n",
      "           2.5203e-01, -1.3327e+00, -1.1064e+00,  3.1460e+00, -5.4399e-01,\n",
      "          -3.8690e-01, -7.1741e-01,  1.4010e+00, -1.4667e-01, -1.6113e+00,\n",
      "          -3.4170e-01,  5.6726e-01, -3.3693e-01, -1.2913e+00, -1.8047e-01,\n",
      "          -1.5065e+00,  2.7809e-01, -1.1406e-01,  7.7884e-01,  1.2044e+00,\n",
      "           5.0460e-01,  1.6944e+00,  4.8412e-01, -6.9009e-01, -6.2880e-01,\n",
      "           3.1567e-01,  1.0856e-01,  4.8191e-01,  7.1773e-01, -2.1605e+00,\n",
      "          -2.3060e-01, -3.2027e-01, -5.1451e-01,  1.8370e+00, -1.2761e+00,\n",
      "           5.9493e-01,  1.1814e-01, -6.1842e-01,  3.0731e+00,  1.3510e+00,\n",
      "          -1.3648e+00,  1.0175e+00, -1.1313e+00, -7.2529e-01,  5.7314e-01,\n",
      "          -3.0224e-01, -2.7425e-01, -4.0786e-01, -9.0578e-01, -6.5483e-01,\n",
      "           2.9639e-01,  7.1701e-01,  5.0258e-01, -9.2444e-01, -1.1584e-01,\n",
      "           6.0851e-01,  3.3892e-01, -2.6982e-01,  4.3617e-01, -8.9052e-02,\n",
      "          -1.3136e+00, -9.6345e-01, -9.5505e-01, -3.6206e-01, -9.7076e-01,\n",
      "           2.0374e+00,  1.5546e+00, -8.2648e-02, -9.1843e-01, -1.0362e-01,\n",
      "           1.1003e+00,  1.5476e+00,  9.4609e-01,  1.6469e+00, -1.7538e-01,\n",
      "          -1.3525e+00,  5.2982e-01,  8.5234e-01, -9.2672e-01,  6.3051e-01,\n",
      "           9.8126e-02,  1.0121e+00, -1.6156e+00, -1.2651e+00, -3.6589e-01,\n",
      "          -1.1974e+00, -7.1240e-01, -1.2269e+00,  1.2416e+00, -6.6188e-01,\n",
      "           5.1655e-01,  6.3565e-01,  4.8431e-01,  1.5030e+00,  1.9601e+00,\n",
      "          -4.3189e-02,  8.6756e-01,  8.2732e-01, -6.0538e-01,  1.8802e-01,\n",
      "          -6.7263e-01,  1.6907e+00, -1.0099e-01,  1.1660e+00,  1.6446e-01,\n",
      "          -4.1602e-02, -1.4350e-02, -7.4963e-01,  6.8729e-01, -1.1490e+00,\n",
      "           4.1312e-01, -1.6840e+00,  5.1210e-02,  1.1107e+00,  7.8005e-01,\n",
      "           1.0391e+00,  8.3084e-01, -3.0655e-01, -5.2314e-01,  1.0808e+00,\n",
      "          -1.2272e+00,  1.4950e+00, -1.5809e-01, -3.2331e-01, -1.9422e-01,\n",
      "           2.2482e+00,  5.5294e-01, -2.9245e-01,  7.5743e-01,  1.1413e+00,\n",
      "           6.4205e-01, -1.4206e-01,  5.6516e-01, -1.9349e+00,  1.0300e+00,\n",
      "           1.0512e+00,  1.2766e-01,  1.2770e-01,  1.9812e+00, -9.4060e-01,\n",
      "           8.8268e-01, -1.4081e+00, -1.7105e+00, -5.1013e-01, -1.1628e+00,\n",
      "           1.2840e-01,  1.3579e+00, -7.7866e-01,  2.4873e-01, -1.6089e-01,\n",
      "           6.6739e-01,  2.2343e-01,  6.2806e-01, -3.2848e-01, -2.8279e-01,\n",
      "           2.4594e-01,  3.0694e-01,  4.8311e-01,  1.3775e-02, -8.7237e-01,\n",
      "           1.7864e-01,  7.9040e-01,  1.2482e+00,  7.5419e-01,  3.9697e-01,\n",
      "           1.3034e+00,  7.7961e-01, -2.5580e-01, -7.2695e-01, -1.0764e+00,\n",
      "          -1.1848e+00,  7.4708e-02, -1.2517e+00,  9.3363e-02,  1.0794e+00,\n",
      "           2.9244e-01, -6.7177e-01, -9.8327e-01,  6.9426e-01,  8.5611e-02,\n",
      "          -1.9200e-01,  8.9693e-01, -9.8618e-01, -1.3292e+00,  1.1207e+00,\n",
      "          -3.7381e-01, -2.2616e-01,  9.8316e-01, -1.2341e+00,  9.1085e-01,\n",
      "           1.1145e+00, -6.6954e-01,  8.5758e-02, -9.0863e-02, -4.7782e-02,\n",
      "           2.8502e-02, -4.5980e-01, -6.0663e-01,  1.8463e+00,  1.7139e-01,\n",
      "           1.9748e+00, -6.2867e-01,  8.4953e-01,  3.3548e-01,  3.9670e-01,\n",
      "          -6.6379e-01, -1.1186e+00, -1.3819e+00,  1.1929e+00, -2.0792e-01,\n",
      "          -5.6655e-01,  1.0730e+00, -1.6123e-01,  3.6796e-01,  4.2062e-01,\n",
      "          -1.0064e+00,  9.3938e-01, -6.4594e-01,  3.4807e-01,  9.1943e-01,\n",
      "          -6.3119e-01, -1.0099e+00,  2.8556e-01, -5.3401e-01, -5.6296e-01,\n",
      "           9.1980e-01, -1.0779e+00,  8.9480e-01,  8.1051e-02, -1.9236e-01,\n",
      "           4.6509e-01, -2.6407e-01, -9.5641e-02, -1.9308e-02,  1.8585e+00,\n",
      "           1.2137e+00,  2.9483e-01, -1.7646e+00,  8.1524e-01, -1.5679e+00,\n",
      "           4.9032e-01, -1.5862e+00,  3.4165e-01, -4.9925e-01, -1.4103e+00,\n",
      "          -4.3046e-01,  1.2857e+00, -9.0783e-01,  4.2161e-01,  1.3991e+00,\n",
      "           7.7247e-01,  2.3808e-01, -1.6024e+00,  5.3190e-01, -2.5623e-01,\n",
      "          -1.9391e-01, -9.6541e-01,  5.3984e-01,  1.1901e+00, -2.5632e+00,\n",
      "          -5.6998e-01,  1.7447e-01,  8.5756e-01, -1.3947e+00, -3.1984e-01,\n",
      "           3.1246e-01,  8.5282e-01,  3.3409e-01,  1.8900e+00,  1.0319e-02,\n",
      "          -2.7540e-04,  4.3889e-01,  2.4810e-01,  9.8589e-01,  4.7093e-01,\n",
      "          -6.9456e-02, -5.5312e-01,  7.1957e-01, -1.2211e+00,  3.1741e-01,\n",
      "           2.5383e+00,  2.3853e+00, -4.4600e-01, -1.6816e+00, -4.4255e-01,\n",
      "           4.9077e-01, -9.1957e-01,  6.1044e-01, -6.3962e-01,  1.4885e-01,\n",
      "          -1.7284e+00, -4.4616e-01, -9.6983e-01,  1.9059e+00,  5.5719e-02,\n",
      "           7.4576e-01, -2.7504e-01, -1.0455e+00,  1.4066e+00, -4.7101e-01,\n",
      "          -2.8503e-01, -2.5365e-01,  5.3788e-02,  1.0709e+00,  1.0771e+00,\n",
      "          -3.6660e-01,  1.0904e+00,  2.9010e-01,  1.5112e+00,  9.7211e-01,\n",
      "           2.7714e-01,  2.4537e-01, -4.6595e-01,  3.5544e-01, -8.4159e-01,\n",
      "          -1.4276e+00,  8.4774e-01,  1.0529e+00,  5.0032e-01,  1.2992e+00,\n",
      "           5.6759e-01,  6.2797e-01, -1.0625e-01,  2.6184e-01, -4.2733e-01,\n",
      "           1.9054e+00,  2.7803e-01, -2.2951e+00,  4.5109e-01,  9.3920e-01,\n",
      "           4.9847e-01,  1.5187e+00]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "gru() received an invalid combination of arguments - got (Tensor, Tensor, list, bool, int, float, bool, int, bool), but expected one of:\n * (Tensor data, Tensor batch_sizes, Tensor hx, tuple of Tensors params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional)\n      didn't match because some of the arguments have invalid types: (Tensor, Tensor, !list!, !bool!, !int!, !float!, !bool!, !int!, bool)\n * (Tensor input, Tensor hx, tuple of Tensors params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first)\n      didn't match because some of the arguments have invalid types: (Tensor, Tensor, !list!, bool, int, float, bool, !int!, bool)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26280\\1605121032.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_chars, dropout_p=0.1).to(device)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdecoder1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecoderRNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrnn_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"gru\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_embedding_dict_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_lang\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_chars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_embedding_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrainIters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.00001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26280\\3650064476.py\u001b[0m in \u001b[0;36mtrainIters\u001b[1;34m(encoder, decoder, n_epochs, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             loss = train(input_tensor, target_tensor, encoder,\n\u001b[1;32m---> 19\u001b[1;33m                         decoder, encoder_optimizer, decoder_optimizer, criterion)\n\u001b[0m\u001b[0;32m     20\u001b[0m             \u001b[0mprint_loss_total\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mplot_loss_total\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26280\\134434465.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mei\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         encoder_output, encoder_hidden = encoder(\n\u001b[1;32m---> 20\u001b[1;33m             input_tensor[ei], encoder_hidden)\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mencoder_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mei\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\siddh\\.virtualenvs\\iitm_CS6910_miteshkhapra_deeplearning-EmLAyZaZ\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26280\\2649366196.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hidden)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedded_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedded_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"lstm\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\siddh\\.virtualenvs\\iitm_CS6910_miteshkhapra_deeplearning-EmLAyZaZ\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\siddh\\.virtualenvs\\iitm_CS6910_miteshkhapra_deeplearning-EmLAyZaZ\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    949\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m             result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[1;32m--> 951\u001b[1;33m                              self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[0;32m    952\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m             result = _VF.gru(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[1;31mTypeError\u001b[0m: gru() received an invalid combination of arguments - got (Tensor, Tensor, list, bool, int, float, bool, int, bool), but expected one of:\n * (Tensor data, Tensor batch_sizes, Tensor hx, tuple of Tensors params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional)\n      didn't match because some of the arguments have invalid types: (Tensor, Tensor, !list!, !bool!, !int!, !float!, !bool!, !int!, bool)\n * (Tensor input, Tensor hx, tuple of Tensors params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first)\n      didn't match because some of the arguments have invalid types: (Tensor, Tensor, !list!, bool, int, float, bool, !int!, bool)\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 512\n",
    "encoder1 = EncoderRNN(rnn_type=\"gru\", input_embedding_dict_size=input_lang.n_chars, input_embedding_size=hidden_size, hidden_size=hidden_size, num_layers=1, bidirectional=1).to(device)\n",
    "# attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_chars, dropout_p=0.1).to(device)\n",
    "decoder1 = DecoderRNN(rnn_type=\"gru\", output_embedding_dict_size=output_lang.n_chars, output_embedding_size=hidden_size, hidden_size=hidden_size, num_layers=3).to(device)\n",
    "trainIters(encoder1, decoder1, 5, print_every=1, learning_rate=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> iyattralaamaa\n",
      "= இயற்றலாமா\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden size (2, 1, 512), got [1, 1, 512]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26280\\4189602464.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mevaluateRandomly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26280\\3982254974.py\u001b[0m in \u001b[0;36mevaluateRandomly\u001b[1;34m(encoder, decoder, n)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'='\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;31m# output_chars, attentions = evaluate(encoder, decoder, pair[0])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0moutput_chars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0moutput_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_chars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'<'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_chars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26280\\1077743545.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(encoder, decoder, word, max_length)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mei\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             encoder_output, encoder_hidden = encoder(input_tensor[ei],\n\u001b[1;32m---> 11\u001b[1;33m                                                      encoder_hidden)\n\u001b[0m\u001b[0;32m     12\u001b[0m             \u001b[0mencoder_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mei\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\siddh\\.virtualenvs\\iitm_CS6910_miteshkhapra_deeplearning-EmLAyZaZ\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26280\\1780282381.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hidden)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0membedded_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedded_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"lstm\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\siddh\\.virtualenvs\\iitm_CS6910_miteshkhapra_deeplearning-EmLAyZaZ\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\siddh\\.virtualenvs\\iitm_CS6910_miteshkhapra_deeplearning-EmLAyZaZ\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    946\u001b[0m             \u001b[0mhx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 948\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    949\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m             result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "\u001b[1;32mc:\\Users\\siddh\\.virtualenvs\\iitm_CS6910_miteshkhapra_deeplearning-EmLAyZaZ\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    230\u001b[0m         \u001b[0mexpected_hidden_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_expected_hidden_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_hidden_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpected_hidden_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpermute_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpermutation\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\siddh\\.virtualenvs\\iitm_CS6910_miteshkhapra_deeplearning-EmLAyZaZ\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mcheck_hidden_size\u001b[1;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[0;32m    224\u001b[0m                           msg: str = 'Expected hidden size {}, got {}') -> None:\n\u001b[0;32m    225\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mexpected_hidden_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpected_hidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected hidden size (2, 1, 512), got [1, 1, 512]"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1cafc67f948>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_chars, attentions = evaluate(\n",
    "    encoder1, attn_decoder1, \"jaavaiyum\")\n",
    "plt.matshow(attentions.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "' '",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13512\\744067545.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mevaluateAndShowAttention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"elle a cinq ans de moins que moi .\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mevaluateAndShowAttention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"elle est trop petit .\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13512\\744067545.py\u001b[0m in \u001b[0;36mevaluateAndShowAttention\u001b[1;34m(input_sentence)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mevaluateAndShowAttention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_sentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     output_words, attentions = evaluate(\n\u001b[1;32m---> 22\u001b[1;33m         encoder1, attn_decoder1, input_sentence)\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'input ='\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_sentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'output ='\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13512\\2208125800.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(encoder, decoder, word, max_length)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMAX_LENGTH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0minput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensorFromword\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_lang\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0minput_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mencoder_hidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitHidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13512\\3460585824.py\u001b[0m in \u001b[0;36mtensorFromword\u001b[1;34m(lang, word)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtensorFromword\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mindexes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexesFromword\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mindexes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEOS_token\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13512\\3460585824.py\u001b[0m in \u001b[0;36mindexesFromword\u001b[1;34m(lang, word)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mindexesFromword\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchar2index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtensorFromword\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13512\\3460585824.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mindexesFromword\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchar2index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtensorFromword\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: ' '"
     ]
    }
   ],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(\n",
    "        encoder1, attn_decoder1, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions)\n",
    "\n",
    "\n",
    "evaluateAndShowAttention(\"elle a cinq ans de moins que moi .\")\n",
    "\n",
    "evaluateAndShowAttention(\"elle est trop petit .\")\n",
    "\n",
    "evaluateAndShowAttention(\"je ne crains pas de mourir .\")\n",
    "\n",
    "evaluateAndShowAttention(\"c est un jeune directeur plein de talent .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iitm_CS6910_miteshkhapra_deeplearning-EmLAyZaZ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
